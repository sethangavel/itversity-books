{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Column Data\n",
    "\n",
    "As part of this module we will explore the functions available under `org.apache.spark.sql.functions` to derive new values from existing column values with in a Data Frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pre-defined Functions\n",
    "\n",
    "We typically process data in the columns using functions in `org.apache.spark.sql.functions`. Let us understand details about these functions in detail as part of this module.\n",
    "* Let us recap about Functions or APIs to process Data Frames.\n",
    " * Projection - `select` or `withColumn`\n",
    " * Filtering - `filter` or `where`\n",
    " * Grouping data by key and perform aggregations - `groupBy`\n",
    " * Sorting data - `sort` or `orderBy` \n",
    "* We can pass column names or literals or expressions to all the Data Frame APIs.\n",
    "* Expressions include arithmetic operations, transformations using functions from `org.apache.spark.sql.functions`.\n",
    "* There are approximately 300 functions under `org.apache.spark.sql.functions`.\n",
    "* We will talk about some of the important functions used for String Manipulation, Date Manipulation etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark Context\n",
    "\n",
    "Let us start spark context for this Notebook so that we can execute the code provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@39d286b4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@39d286b4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Processing Column Data\").\n",
    "    //master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@39d286b4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Data Frame\n",
    "Let us go ahead and create dummy data from to explore functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(X)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(X)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Oracle dual (view)\n",
    "// dual - dummy CHAR(1)\n",
    "// \"X\" - One record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once Data Frame is created, we can use to understand how to use functions. For example, to get current date, we can run `df.select(current_date()).show()`.\n",
    "\n",
    "It is similar to Oracle Query `SELECT sysdate FROM dual`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(X)\n",
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"X\")\n",
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2020-05-25|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2020-05-25|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date.alias(\"current_date\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another example of creating Data Frame using collection of employees. We will be using this Data Frame to explore all the important functions to process column data in detail.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees = List((1,Scott,Tiger,1000.0,United States,+1 123 456 7890,123 45 6789), (2,Henry,Ford,1250.0,India,+91 234 567 8901,456 78 9123), (3,Nick,Junior,750.0,United Kingdom,+44 111 111 1111,222 33 4444), (4,Bill,Gomes,1500.0,Australia,+61 987 654 3210,789 12 6118))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((1,Scott,Tiger,1000.0,United States,+1 123 456 7890,123 45 6789), (2,Henry,Ford,1250.0,India,+91 234 567 8901,456 78 9123), (3,Nick,Junior,750.0,United Kingdom,+44 111 111 1111,222 33 4444), (4,Bill,Gomes,1500.0,Australia,+61 987 654 3210,789 12 6118))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"United States\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"United Kingdom\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"Australia\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employees.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// employees.count for DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.IllegalArgumentException\n",
       "Message: requirement failed: The number of columns doesn't match.\n",
       "Old column names (7): _1, _2, _3, _4, _5, _6, _7\n",
       "New column names (1): field_1\n",
       "StackTrace: Old column names (7): _1, _2, _3, _4, _5, _6, _7\n",
       "New column names (1): field_1\n",
       "  at scala.Predef$.require(Predef.scala:224)\n",
       "  at org.apache.spark.sql.Dataset.toDF(Dataset.scala:447)\n",
       "  at org.apache.spark.sql.DatasetHolder.toDF(DatasetHolder.scala:44)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.toDF(\"field_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 5 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = false)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|nationality   |phone_number    |ssn        |\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|1          |Scott     |Tiger    |1000.0|United States |+1 123 456 7890 |123 45 6789|\n",
      "|2          |Henry     |Ford     |1250.0|India         |+91 234 567 8901|456 78 9123|\n",
      "|3          |Nick      |Junior   |750.0 |United Kingdom|+44 111 111 1111|222 33 4444|\n",
      "|4          |Bill      |Gomes    |1500.0|Australia     |+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories of Functions\n",
    "\n",
    "There are approximately 300 functions under org.apache.spark.sql.functions. At a higher level they can be grouped into a few categories.\n",
    "* String Manipulation Functions\n",
    " * Case Conversion - `lower`,  `upper`\n",
    " * Getting Length -  `length`\n",
    " * Extracting substrings - `substring`, `split`\n",
    " * Trimming - `trim`, `ltrim`, `rtrim`\n",
    " * Padding - `lpad`, `rpad`\n",
    " * Concatenating strings - `concat`\n",
    "* Date Manipulation Functions\n",
    " * Getting current date and time - `current_date`, `current_timestamp`\n",
    " * Date Arithmetic - `date_add`, `date_sub`, `datediff`, `months_between`, `add_months`, `next_day`\n",
    " * Beginning and Ending Date or Time - `last_day`, `trunc`, `date_trunc`\n",
    " * Formatting Date - `date_format`\n",
    " * Extracting Information - `dayofyear`, `dayofmonth`, `dayofweek`, `year`, `month`\n",
    "* Aggregate Functions\n",
    " * `count`, `countDistinct`\n",
    " * `sum`, `avg`\n",
    " * `min`, `max`\n",
    "* Other Functions - We will explore depending on the use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Functions - col and lit\n",
    "\n",
    "Let us understand special functions such as col and lit.\n",
    "\n",
    "* First let us create Data Frame for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_id: integer (nullable = false)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- salary: double (nullable = false)\n",
      " |-- nationality: string (nullable = true)\n",
      " |-- phone_number: string (nullable = true)\n",
      " |-- ssn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Data Frame APIs such as `select`, `groupBy`, `orderBy` etc we can pass column names as strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "// to use operators such as $ in place of functions like col\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select($\"first_name\", $\"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Alternative using col function\n",
    "// $ is shorthand operator for col from implicits\n",
    "import org.apache.spark.sql.functions.col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(col(\"first_name\"), $\"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Alternative by passing column names as strings.\n",
    "employeesDF.\n",
    "    select(\"first_name\", \"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|     Scott|    Tiger|\n",
      "|     Henry|     Ford|\n",
      "|      Nick|   Junior|\n",
      "|      Bill|    Gomes|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// We have to pass all the column names as strings or column type (using col or $)\n",
    "// This will not work\n",
    "employeesDF.\n",
    "    select($\"first_name\", $\"last_name\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If there are no transformations on any column in any function then we should be able to pass all column names as strings.\n",
    "* If not we need to pass all columns as type column by using col function or its shorthand operator $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|   nationality|count|\n",
      "+--------------+-----+\n",
      "| United States|    1|\n",
      "|         India|    1|\n",
      "|     Australia|    1|\n",
      "|United Kingdom|    1|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Passing columns as part of groupBy\n",
    "employeesDF.\n",
    "    groupBy(\"nationality\").\n",
    "    count.\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|upper(nationality)|count|\n",
      "+------------------+-----+\n",
      "|    UNITED KINGDOM|    1|\n",
      "|             INDIA|    1|\n",
      "|         AUSTRALIA|    1|\n",
      "|     UNITED STATES|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    groupBy(upper($\"nationality\")).\n",
    "    count.\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| United States| +1 123 456 7890|123 45 6789|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|United Kingdom|+44 111 111 1111|222 33 4444|\n",
      "|          4|      Bill|    Gomes|1500.0|     Australia|+61 987 654 3210|789 12 6118|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Passing columns as part of orderBy or sort\n",
    "employeesDF.\n",
    "    orderBy(\"employee_id\").\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, if we want to apply any transformation using functions then passing column names as strings to some of the functions will not suffice. We have to pass them as column type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:43: error: type mismatch;\n",
       " found   : String(\"first_name\")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(upper(\"first_name\")).\n",
       "                        ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//This code fails as upper is not valid function on string\n",
    "employeesDF.\n",
    "    select(upper(\"first_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `col` is the function which will convert column name from string type to **Column** type. We can also refer column names as **Column** type using Data Frame name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|upper(first_name)|\n",
      "+-----------------+\n",
      "|            SCOTT|\n",
      "|            HENRY|\n",
      "|             NICK|\n",
      "|             BILL|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Using col and upper\n",
    "employeesDF.\n",
    "    select(upper(col(\"first_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|upper(first_name)|\n",
      "+-----------------+\n",
      "|            SCOTT|\n",
      "|            HENRY|\n",
      "|             NICK|\n",
      "|             BILL|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Alternate using $ and upper\n",
    "employeesDF.\n",
    "    select(upper($\"first_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|upper(nationality)|count|\n",
      "+------------------+-----+\n",
      "|    UNITED KINGDOM|    1|\n",
      "|             INDIA|    1|\n",
      "|         AUSTRALIA|    1|\n",
      "|     UNITED STATES|    1|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Using as part of groupBy\n",
    "employeesDF.\n",
    "    groupBy(upper($\"nationality\")).\n",
    "    count.\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     Australia|+61 987 654 3210|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|United Kingdom|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| United States| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Using as part of orderBy\n",
    "employeesDF.\n",
    "    orderBy(upper($\"nationality\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "|          4|      Bill|    Gomes|1500.0|     Australia|+61 987 654 3210|789 12 6118|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123|\n",
      "|          3|      Nick|   Junior| 750.0|United Kingdom|+44 111 111 1111|222 33 4444|\n",
      "|          1|     Scott|    Tiger|1000.0| United States| +1 123 456 7890|123 45 6789|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Alternative - we can also refer column names using Data Frame like this\n",
    "employeesDF.\n",
    "    orderBy(upper(employeesDF(\"nationality\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sometimes, we want to add a literal to the column values. For example, we might want to concatenate first_name and last_name with separated by comma and space in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "// Below approaches fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:43: error: type mismatch;\n",
       " found   : String(\",\")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(concat($\"first_name\", \",\", $\"last_name\")).\n",
       "                                        ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat($\"first_name\", \",\", $\"last_name\")).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:44: error: type mismatch;\n",
       " found   : String(\", \")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(concat(col(\"first_name\"), \", \", col(\"last_name\"))).\n",
       "                                            ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Same as above\n",
    "employeesDF.\n",
    "    select(concat(col(\"first_name\"), \", \", col(\"last_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:44: error: type mismatch;\n",
       " found   : String(\", \")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(concat(employeesDF(\"first_name\"), \", \", employeesDF(\"last_name\"))).\n",
       "                                                    ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Referring columns using Data Frame\n",
    "employeesDF.\n",
    "    select(concat(employeesDF(\"first_name\"), \", \", employeesDF(\"last_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:43: error: type mismatch;\n",
       " found   : String(\"first_name\")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(concat(\"first_name\", \", \", \"last_name\")).\n",
       "                         ^\n",
       "<console>:43: error: type mismatch;\n",
       " found   : String(\", \")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(concat(\"first_name\", \", \", \"last_name\")).\n",
       "                                       ^\n",
       "<console>:43: error: type mismatch;\n",
       " found   : String(\"last_name\")\n",
       " required: org.apache.spark.sql.Column\n",
       "           select(concat(\"first_name\", \", \", \"last_name\")).\n",
       "                                             ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat(\"first_name\", \", \", \"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we pass the literals directly in the form of string or numeric type, then it will fail. We have to convert literals to column type by using `lit` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "// Using lit to use literals to derive new expressions\n",
    "import org.apache.spark.sql.functions.{concat, col, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|concat(first_name, , , last_name)|\n",
      "+---------------------------------+\n",
      "|                     Scott, Tiger|\n",
      "|                      Henry, Ford|\n",
      "|                     Nick, Junior|\n",
      "|                      Bill, Gomes|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat(col(\"first_name\"), lit(\", \"), col(\"last_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|concat(first_name, , , last_name)|\n",
      "+---------------------------------+\n",
      "|                     Scott, Tiger|\n",
      "|                      Henry, Ford|\n",
      "|                     Nick, Junior|\n",
      "|                      Bill, Gomes|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(concat($\"first_name\", lit(\", \"), employeesDF(\"last_name\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - Case Conversion and Length\n",
    "Let us check the functions which can convert the case of the column values which are of type string and also get the length.\n",
    "* Convert all the alphabetic characters in a string to **uppercase** - `upper`\n",
    "* Convert all the alphabetic characters in a string to **lowercase** - `lower`\n",
    "* Convert first character in a string to **uppercase** - `initcap`\n",
    "* Get **number of characters in a string** - `length`\n",
    "* All the 4 functions take column type argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform tasks to understand the behavior of case conversion functions and length.\n",
    "\n",
    "* Use employees data and create a Data Frame.\n",
    "* Apply all 4 functions on **nationality** and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, upper, lower, initcap, length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|employee_id|   nationality|nationality_upper|nationality_lower|nationality_initcap|nationality_length|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "|          1| United States|    UNITED STATES|    united states|      United States|                13|\n",
      "|          2|         India|            INDIA|            india|              India|                 5|\n",
      "|          3|United Kingdom|   UNITED KINGDOM|   united kingdom|     United Kingdom|                14|\n",
      "|          4|     Australia|        AUSTRALIA|        australia|          Australia|                 9|\n",
      "+-----------+--------------+-----------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"nationality\").\n",
    "    withColumn(\"nationality_upper\", upper(col(\"nationality\"))).\n",
    "    withColumn(\"nationality_lower\", lower($\"nationality\")).\n",
    "    withColumn(\"nationality_initcap\", initcap(employeesDF(\"nationality\"))).\n",
    "    withColumn(\"nationality_length\", length(col(\"nationality\"))).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - substring\n",
    "\n",
    "Let us understand how we can extract substrings using function  `substring`.\n",
    "* If we are processing **fixed length columns** then we use `substring` to extract the information.\n",
    "* Here are some of the examples for **fixed length columns** and the use cases for which we typically extract information..\n",
    " * 9 Digit Social Security Number. We typically extract last 4 digits and provide it to the tele verification applications..\n",
    " * 16 Digit Credit Card Number. We typically use first 4 digit number to identify Credit Card Provider and last 4 digits for the purpose of tele verification.\n",
    " * Data coming from MainFrames systems are quite often fixed length. We might have to extract the information and store in multiple columns.\n",
    "* `substring` function takes 3 arguments, **column**, **position**, **length**. We can also provide position from the end by passing negative value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s = Hello World\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Hello World"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s = \"Hello World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.substring(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ell"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.substring(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "World"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.substring(6, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(X)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(X)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dummy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|dummy|\n",
      "+-----+\n",
      "|    X|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{substring, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|substring(Hello World, 7, 5)|\n",
      "+----------------------------+\n",
      "|                       World|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), 7, 5)).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|substring(Hello World, -5, 5)|\n",
      "+-----------------------------+\n",
      "|                        World|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(substring(lit(\"Hello World\"), -5, 5)).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform few tasks to extract information from fixed length strings.\n",
    "* Create a list for employees with name, ssn and phone_number.\n",
    "* SSN Format **3 2 4** - Fixed Length with 9 digits\n",
    "* Phone Number Format - Country Code is variable and remaining phone number have 10 digits:\n",
    " * Country Code - one to 3 digits\n",
    " * Area Code - 3 digits\n",
    " * Phone Number Prefix - 3 digits\n",
    " * Phone Number Remaining - 4 digits\n",
    " * All the 4 parts are separated by spaces\n",
    "* Create a Dataframe with column names name, ssn and phone_number\n",
    "* Extract last 4 digits from the phone number.\n",
    "* Extract last 4 digits from SSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"phone_number\", \"ssn\").\n",
    "    withColumn(\"phone_last4\", substring($\"phone_number\", -4, 4).cast(\"int\")).\n",
    "    withColumn(\"ssn_last4\", substring($\"ssn\", 8, 4).cast(\"int\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|       3210|     6118|\n",
      "+-----------+----------------+-----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select($\"employee_id\", $\"phone_number\", $\"ssn\", \n",
    "           substring($\"phone_number\", -4, 4).cast(\"int\").alias(\"phone_last4\"),\n",
    "           substring($\"ssn\", 8, 4).cast(\"int\").alias(\"ssn_last4\")\n",
    "          ).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - split\n",
    "Let us understand how we can extract substrings using  `split`.\n",
    "* If we are processing **variable length columns** with **delimiter** then we use `split` to extract the information.\n",
    "* Here are some of the examples for **variable length columns** and the use cases for which we typically extract information.\n",
    " * Address where we store House Number, Street Name, City, State and Zip Code comma separated. We might want to extract City and State for demographics reports.\n",
    "* `split` takes 2 arguments, **column** and **delimiter**.\n",
    "* `split` convert each string into array and we can access the elements using index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(X)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(X)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{split, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|split(Hello World, how are you,  )|\n",
      "+----------------------------------+\n",
      "|[Hello, World,, how, are, you]    |\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+\n",
      "|split(Hello World, how are you,  )[2]|\n",
      "+-------------------------------------+\n",
      "|how                                  |\n",
      "+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(lit(\"Hello World, how are you\"), \" \")(2)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the problems can be solved either by using `substring` or `split`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Let us perform few tasks to extract information from fixed length strings as well as delimited variable length strings.\n",
    "* Create a list for employees with name, ssn and phone_number.\n",
    "* SSN Format **3 2 4** - Fixed Length with 9 digits\n",
    "* Phone Number Format - Country Code is variable and remaining phone number have 10 digits:\n",
    " * Country Code - one to 3 digits\n",
    " * Area Code - 3 digits\n",
    " * Phone Number Prefix - 3 digits\n",
    " * Phone Number Remaining - 4 digits\n",
    " * All the 4 parts are separated by spaces\n",
    "* Create a Dataframe with column names name, ssn and phone_number\n",
    "* Extract area code and last 4 digits from the phone number.\n",
    "* Extract last 4 digits from SSN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employees = List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 45 6789), (2,Henry,Ford,1250.0,India,+91 234 567 8901,456 78 9123), (3,Nick,Junior,750.0,united KINGDOM,+44 111 111 1111,222 33 4444), (4,Bill,Gomes,1500.0,AUSTRALIA,+61 987 654 3210,789 12 6118))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((1,Scott,Tiger,1000.0,united states,+1 123 456 7890,123 45 6789), (2,Henry,Ford,1250.0,India,+91 234 567 8901,456 78 9123), (3,Nick,Junior,750.0,united KINGDOM,+44 111 111 1111,222 33 4444), (4,Bill,Gomes,1500.0,AUSTRALIA,+61 987 654 3210,789 12 6118))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesDF = [employee_id: int, first_name: string ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 5 more fields]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "|employee_id|    phone_number|        ssn|area_code|phone_last4|ssn_last4|\n",
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "|          1| +1 123 456 7890|123 45 6789|      123|       7890|     6789|\n",
      "|          2|+91 234 567 8901|456 78 9123|      234|       8901|     9123|\n",
      "|          3|+44 111 111 1111|222 33 4444|      111|       1111|     4444|\n",
      "|          4|+61 987 654 3210|789 12 6118|      987|       3210|     6118|\n",
      "+-----------+----------------+-----------+---------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    select(\"employee_id\", \"phone_number\", \"ssn\").\n",
    "    withColumn(\"area_code\", split($\"phone_number\", \" \")(1).cast(\"int\")).\n",
    "    withColumn(\"phone_last4\", split($\"phone_number\", \" \")(3).cast(\"int\")).\n",
    "    withColumn(\"ssn_last4\", split($\"ssn\", \" \")(2).cast(\"int\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    select($\"employee_id\", $\"phone_number\", $\"ssn\", \n",
    "           split($\"phone_number\", \" \")(1).cast(\"int\").alias(\"area_code\"),\n",
    "           split($\"phone_number\", \" \")(3).cast(\"int\").alias(\"phone_last\"),\n",
    "           split($\"ssn\", \" \")(2).cast(\"int\").alias(\"ssn_last4\")\n",
    "          ).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - Concatenating of Strings\n",
    "Let us understand how to concatenate strings using `concat` function.\n",
    "* We can pass a variable number of strings to `concat` function.\n",
    "* It will return one string concatenating all the strings.\n",
    "* If we have to concatenate literal in between the strings we have to use `lit` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform few tasks to understand more about \n",
    "`concat` function.\n",
    "* Lets create a Data Frame and explore `concat` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "employeesDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a new column by name **full_name** concatenating **first_name** and **last_name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----------+\n",
      "|employee_id|first_name|last_name|salary|   nationality|    phone_number|        ssn|  full_name|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----------+\n",
      "|          1|     Scott|    Tiger|1000.0| united states| +1 123 456 7890|123 45 6789|Scott Tiger|\n",
      "|          2|     Henry|     Ford|1250.0|         India|+91 234 567 8901|456 78 9123| Henry Ford|\n",
      "|          3|      Nick|   Junior| 750.0|united KINGDOM|+44 111 111 1111|222 33 4444|Nick Junior|\n",
      "|          4|      Bill|    Gomes|1500.0|     AUSTRALIA|+61 987 654 3210|789 12 6118| Bill Gomes|\n",
      "+-----------+----------+---------+------+--------------+----------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", concat($\"first_name\",lit(' '), $\"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Improvise by adding a **comma followed by a space** in between **first_name** and **last_name**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{concat, lit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "employeesDF.\n",
    "    withColumn(\"full_name\", concat($\"first_name\", lit(\", \"), $\"last_name\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - Padding\n",
    "Let us understand how to pad characters at the beginning or at the end of strings.\n",
    "* We typically pad characters to build fixed length values or records.\n",
    "* Fixed length values or records are extensively used in Mainframes based systems.\n",
    "* Length of each and every field in fixed length records is predetermined and if the value of the field is less than the predetermined length then we pad with a standard character.\n",
    "* In terms of numeric fields we pad with zero on the leading or left side. For non numeric fields, we pad with some standard character on trailing or right side.\n",
    "* We use `lpad` to pad a string with a specific character on leading or left side and `rpad` to pad on trailing or right side.\n",
    "* Both lpad and rpad, take 3 arguments - column or expression, desired length and the character need to be padded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform simple tasks to understand the syntax of `lpad` or `rpad`.\n",
    "* Create a Dataframe with single value and single column.\n",
    "* Apply `lpad` to pad with - to Hello to make it 10 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lit, lpad}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(lpad(lit(\"Hello\"), 10, \"-\").alias(\"dummy\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.select(lpad(lit(2), 2, \"0\").alias(\"dummy\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform the task to understand how to use pad functions to convert our data into fixed length records.\n",
    "\n",
    "* Lets take the **employees** Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employees = List((1, \"Scott\", \"Tiger\", 1000.0, \n",
    "                      \"united states\", \"+1 123 456 7890\", \"123 45 6789\"\n",
    "                     ),\n",
    "                     (2, \"Henry\", \"Ford\", 1250.0, \n",
    "                      \"India\", \"+91 234 567 8901\", \"456 78 9123\"\n",
    "                     ),\n",
    "                     (3, \"Nick\", \"Junior\", 750.0, \n",
    "                      \"united KINGDOM\", \"+44 111 111 1111\", \"222 33 4444\"\n",
    "                     ),\n",
    "                     (4, \"Bill\", \"Gomes\", 1500.0, \n",
    "                      \"AUSTRALIA\", \"+61 987 654 3210\", \"789 12 6118\"\n",
    "                     )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val employeesDF = employees.\n",
    "    toDF(\"employee_id\", \"first_name\",\n",
    "         \"last_name\", \"salary\",\n",
    "         \"nationality\", \"phone_number\",\n",
    "         \"ssn\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use **pad** functions to convert each of the field into fixed length and concatenate. Here are the details for each of the fields.\n",
    " * Length of the employee_id should be 5 characters and should be padded with zero.\n",
    " * Length of first_name and last_name should be 10 characters and should be padded with - on the right side.\n",
    " * Length of salary should be 10 characters and should be padded with zero.\n",
    " * Length of the nationality should be 15 characters and should be padded with - on the right side.\n",
    " * Length of the phone_number should be 17 characters and should be padded with - on the right side.\n",
    " * Length of the ssn can be left as is. It is 11 characters.\n",
    " \n",
    "* Create a new Dataframe **empFixedDF** with column name **employee**. Preview the data by disabling truncate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{lpad, rpad, concat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "empFixedDF = [employee: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee: string]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val empFixedDF = employeesDF.select(\n",
    "    concat(\n",
    "        lpad($\"employee_id\", 5, \"0\"),\n",
    "        rpad($\"first_name\", 10, \"-\"),\n",
    "        rpad($\"last_name\", 10, \"-\"),\n",
    "        lpad($\"salary\", 10, \"0\"),\n",
    "        rpad($\"nationality\", 15, \"-\"),\n",
    "        rpad($\"phone_number\", 17, \"-\"),\n",
    "        $\"ssn\"\n",
    "    ).alias(\"employee\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+\n",
      "|employee                                                                      |\n",
      "+------------------------------------------------------------------------------+\n",
      "|00001Scott-----Tiger-----00001000.0united states--+1 123 456 7890--123 45 6789|\n",
      "|00002Henry-----Ford------00001250.0India----------+91 234 567 8901-456 78 9123|\n",
      "|00003Nick------Junior----00000750.0united KINGDOM-+44 111 111 1111-222 33 4444|\n",
      "|00004Bill------Gomes-----00001500.0AUSTRALIA------+61 987 654 3210-789 12 6118|\n",
      "+------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empFixedDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation - Trimming\n",
    "Let us understand how to trim unwanted leading and trailing characters around a string.\n",
    "* We typically use trimming to remove unnecessary characters from fixed length records.\n",
    "* Fixed length records are extensively used in Mainframes and we might have to process it using Spark.\n",
    "* As part of processing we might want to remove leading or trailing characters such as 0 in case of numeric types and space or some standard character in case of alphanumeric types.\n",
    "* As of now Spark trim functions take the column as argument and remove leading or trailing spaces.\n",
    "* Trim spaces towards left - `ltrim`\n",
    "* Trim spaces towards right - `rtrim`\n",
    "* Trim spaces on both sides - `trim`\n",
    "* We can also trim other characters than spaces using these trim functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us understand how to use trim functions to remove spaces on left or right or both.\n",
    "* Create a Dataframe with one column and one record.\n",
    "* Apply trim functions to trim spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "l = List(\"   Hello.    \")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(\"   Hello.    \")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val l = List(\"   Hello.    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [dummy: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[dummy: string]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{col, ltrim, rtrim, trim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+-----+------+\n",
      "|        dummy|     ltrim|   rtrim| trim|trimzd|\n",
      "+-------------+----------+--------+-----+------+\n",
      "|   Hello.    |Hello.    |   Hello|Hello|Hello.|\n",
      "+-------------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"ltrim\", ltrim(col(\"dummy\"))).\n",
    "    withColumn(\"rtrim\", rtrim(rtrim(col(\"dummy\")), \".\")).\n",
    "    withColumn(\"trim\", trim(trim(col(\"dummy\")), \".\")).\n",
    "    withColumn(\"trimzd\", trim(trim(col(\"dummy\"),\".\"), \" \")).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+\n",
      "|        dummy|       AsItis|       trim_d|\n",
      "+-------------+-------------+-------------+\n",
      "|   Hello.    |   Hello.    |   Hello.    |\n",
      "+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"AsItis\", col(\"dummy\")).\n",
    "    withColumn(\"trim_d\", trim(trim(col(\"dummy\"),\".\"),\".\")).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - Overview\n",
    "Let us get an overview about Date and Time using available functions.\n",
    "* We can use `current_date` to get todays server date. \n",
    " * Date will be returned using **yyyy-MM-dd** format.\n",
    "* We can use `current_timestamp` to get current server time. \n",
    " * Timestamp will be returned using **yyyy-MM-dd HH:mm:ss.SSS** format.\n",
    " * Hours will be by default in 24 hour format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2020-05-25|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_date.alias(\"current_date\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|current_time           |\n",
      "+-----------------------+\n",
      "|2020-05-25 17:10:03.849|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(current_timestamp.alias(\"current_time\")).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - Arithmetic\n",
    "Let us perform Date and Time Arithmetic using relevant functions.\n",
    "* Adding days to a date or timestamp - `date_add`\n",
    "* Subtracting days from a date or timestamp - `date_sub`\n",
    "* Getting difference between 2 dates or timestamps - `datediff`\n",
    "* Getting a number of months between 2 dates or timestamps - `months_between`\n",
    "* Adding months to a date or timestamp - `add_months`\n",
    "* Getting next day from a given date - `next_day`\n",
    "* All the functions are self explanatory. We can apply these on standard date or timestamp. All the functions return date even when applied on timestamp field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform some tasks related to date arithmetic.\n",
    "* Get help on each and every function first and understand what all arguments need to be passed.\n",
    "* Create a Dataframe by name datetimesDF with columns date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF = [date: string, time: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, time: string]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add 10 days to both date and time values.\n",
    "* Subtract 10 days from both date and time values.\n",
    "* Get the difference between current_date and date values as well as current_timestamp and time values.\n",
    "* Get the number of months between current_date and date values as well as current_timestamp and time values.\n",
    "* Add 3 months to both date values as well as time values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{date_add, date_sub}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+-------------+-------------+---------------+\n",
      "|date      |time                   |date_add_date|date_add_time|date_sub_date|date_sub_time|date_sub_time_1|\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-03-10   |2014-03-10   |2014-02-18   |2014-02-18   |2014-03-10     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-03-10   |2016-03-10   |2016-02-19   |2016-02-19   |2016-03-10     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-11-10   |2018-01-10   |2017-10-21   |2017-12-21   |2018-01-10     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-12-10   |2019-09-10   |2019-11-20   |2019-08-21   |2019-09-10     |\n",
      "+----------+-----------------------+-------------+-------------+-------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_add_date\", date_add($\"date\", 10)).\n",
    "    withColumn(\"date_add_time\", date_add($\"time\", 10)).\n",
    "    withColumn(\"date_sub_date\", date_sub($\"date\", 10)).\n",
    "    withColumn(\"date_sub_time\", date_sub($\"time\", 10)).\n",
    "withColumn(\"date_sub_time_1\", date_sub($\"time\", -10)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{current_date, current_timestamp, datediff}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+-------------+\n",
      "|date      |time                   |datediff_date|datediff_time|\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2278         |2278         |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|1547         |1547         |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|937          |876          |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|177          |268          |\n",
      "+----------+-----------------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"datediff_date\", datediff(current_date, $\"date\")).\n",
    "    withColumn(\"datediff_time\", datediff(current_timestamp, $\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{months_between, add_months, round}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|date      |time                   |months_between_date|months_between_time|add_months_date|add_months_time|\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|74.90323           |74.91              |2014-05-31     |2014-05-31     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|50.87097           |50.88              |2016-05-31     |2016-05-31     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|30.80645           |28.81              |2018-01-31     |2018-03-31     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|5.83871            |8.83               |2020-02-29     |2019-11-30     |\n",
      "+----------+-----------------------+-------------------+-------------------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"months_between_date\", round(months_between(current_date, $\"date\"), 5)).\n",
    "    withColumn(\"months_between_time\", round(months_between(current_timestamp, $\"time\"), 2)).  \n",
    "    withColumn(\"add_months_date\", add_months($\"date\", 3)).\n",
    "    withColumn(\"add_months_time\", add_months($\"time\", 3)).\n",
    "//withColumn(\"months_between_date_1\", add_months($\"months_between_date\", 3)).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - trunc and date_trunc\n",
    "In Data Warehousing we quite often run to date reports such as week to date, month to date, year to date etc.\n",
    "* We can use `trunc` or `date_trunc` for the same to get the beginning date of the week, month, current year etc by passing date or timestamp to it.\n",
    "* We can use `trunc` to get beginning date of the month or year by passing date or timestamp to it - for example `trunc(current_date(), \"MM\")` will give the first of the current month.\n",
    "* We can use `date_trunc` to get beginning date of the month or year as well as beginning time of the day or hour by passing timestamp to it.\n",
    " * Get beginning date based on month - `date_trunc(\"MM\", current_timestamp())`\n",
    " * Get beginning time based on day - `date_trunc(\"DAY\", current_timestamp())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform few tasks to understand trunc and date_trunc in detail.\n",
    "* Create a Dataframe by name datetimesDF with columns date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF = [date: string, time: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, time: string]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get beginning month date using date field and beginning year date using time field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_trunc|time_trunc|\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-01|2014-01-01|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-01|2016-01-01|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-01|2017-01-01|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-01|2019-01-01|\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_trunc\", trunc($\"date\", \"MM\")).\n",
    "    withColumn(\"time_trunc\", trunc($\"time\", \"yyyy\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get beginning hour time using date and time field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|date      |time                   |date_dt            |time_dt            |\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014-02-28 00:00:00|2014-02-28 10:00:00|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016-02-29 00:00:00|2016-02-29 08:00:00|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017-10-31 00:00:00|2017-12-31 11:00:00|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019-11-30 00:00:00|2019-08-31 00:00:00|\n",
      "+----------+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_dt\", date_trunc(\"HOUR\", $\"date\")).\n",
    "    withColumn(\"time_dt\", date_trunc(\"HOUR\", $\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date and Time - Extracting Information\n",
    "\n",
    "Let us understand how to extract information from dates or times using functions.\n",
    "\n",
    "* We can use date_format to extract the required information in a desired format from date or timestamp.\n",
    "* There are also specific functions to extract year, month, day with in a week, a day with in a month, day with in a year etc.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Let us perform few tasks to extract the information we need from date or timestamp.\n",
    "\n",
    "* Create a Dataframe by name datetimesDF with columns date and time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes = List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((2014-02-28,2014-02-28 10:00:00.123), (2016-02-29,2016-02-29 08:08:08.999), (2017-10-31,2017-12-31 11:59:59.123), (2019-11-30,2019-08-31 00:00:00.000))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((\"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (\"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (\"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (\"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF = [date: string, time: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, time: string]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |time                   |\n",
      "+----------+-----------------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get year from fields date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+---------+---------+\n",
      "|date      |time                   |date_year|time_year|\n",
      "+----------+-----------------------+---------+---------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2014     |2014     |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2016     |2016     |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|2017     |2017     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|2019     |2019     |\n",
      "+----------+-----------------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_year\", year($\"date\")).\n",
    "    withColumn(\"time_year\", year($\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get one or two digit month from fields date and time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+\n",
      "|date      |time                   |date_month|time_month|\n",
      "+----------+-----------------------+----------+----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|2         |2         |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2         |2         |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|10        |12        |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|11        |8         |\n",
      "+----------+-----------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_month\", month($\"date\")).\n",
    "    withColumn(\"time_month\", month($\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get year and month in yyyyMM format from date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------+-------+\n",
      "|date      |time                   |date_ym|time_ym|\n",
      "+----------+-----------------------+-------+-------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|201402 |201402 |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|201602 |201602 |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|201710 |201712 |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|201911 |201908 |\n",
      "+----------+-----------------------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_ym\", date_format($\"date\", \"yyyyMM\")).\n",
    "    withColumn(\"time_ym\", date_format($\"time\", \"yyyyMM\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get day with in a week, a day with in a month and day within a year from date and time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val l = List(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = l.toDF(\"dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{dayofweek, current_date, dayofmonth, dayofyear}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|dayofweek(current_date())|\n",
      "+-------------------------+\n",
      "|                        2|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(dayofweek(current_date)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------+--------+--------+--------+--------+--------+\n",
      "|date      |time                   |date_dow|time_dow|date_dom|time_dom|date_doy|time_doy|\n",
      "+----------+-----------------------+--------+--------+--------+--------+--------+--------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|6       |6       |28      |28      |59      |59      |\n",
      "|2016-02-29|2016-02-29 08:08:08.999|2       |2       |29      |29      |60      |60      |\n",
      "|2017-10-31|2017-12-31 11:59:59.123|3       |1       |31      |31      |304     |365     |\n",
      "|2019-11-30|2019-08-31 00:00:00.000|7       |7       |30      |31      |334     |243     |\n",
      "+----------+-----------------------+--------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_dow\", dayofweek($\"date\")).\n",
    "    withColumn(\"time_dow\", dayofweek($\"time\")).\n",
    "    withColumn(\"date_dom\", dayofmonth($\"date\")).\n",
    "    withColumn(\"time_dom\", dayofmonth($\"time\")).\n",
    "    withColumn(\"date_doy\", dayofyear($\"date\")).\n",
    "    withColumn(\"time_doy\", dayofyear($\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get the information from time in yyyyMMddHHmmss format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------+\n",
      "|date      |time                   |time_ts       |\n",
      "+----------+-----------------------+--------------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|20140228100000|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|20160229080808|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|20171231115959|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|20190831000000|\n",
      "+----------+-----------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"time_ts\", date_format($\"time\", \"yyyyMMddHHmmss\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtf = [date: string, time: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date: string, time: string ... 1 more field]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dtf = datetimesDF.withColumn(\"time_ts\", date_format($\"time\", \"yyyyMMddHHmmss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- time_ts: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+-----------+\n",
      "|date      |time                   |date_us   |date_f     |\n",
      "+----------+-----------------------+----------+-----------+\n",
      "|2014-02-28|2014-02-28 10:00:00.123|02-28-2014|28-Feb-2014|\n",
      "|2016-02-29|2016-02-29 08:08:08.999|02-29-2016|29-Feb-2016|\n",
      "|2017-10-31|2017-12-31 11:59:59.123|10-31-2017|31-Oct-2017|\n",
      "|2019-11-30|2019-08-31 00:00:00.000|11-30-2019|30-Nov-2019|\n",
      "+----------+-----------------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"date_us\", date_format($\"date\", \"MM-dd-yyyy\")).\n",
    "    withColumn(\"date_f\", date_format($\"date\", \"dd-MMM-yyyy\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Unix Timestamp\n",
    "\n",
    "Let us understand how to deal with Unix Timestamp in Spark.\n",
    "\n",
    "* It is an integer and started from January 1st 1970 Midnight UTC.\n",
    "* Beginning time is also known as epoch and is incremented by 1 every second.\n",
    "* We can convert Unix Timestamp to regular date or timestamp and vice versa.\n",
    "* We can use `unix_timestamp` to convert regular date or timestamp to a unix timestamp value. For example `unix_timestamp(lit(\"2019-11-19 00:00:00\"))`\n",
    "* We can use `from_unixtime` to convert unix timestamp to regular date or timestamp. For example `from_unixtime(lit(1574101800))`\n",
    "* We can also pass format to both the functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us perform few tasks to understand how to deal with Unix Timestamp.\n",
    "\n",
    "*   Create a Dataframe by name datetimesDF with columns dateid, date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimes = List((20140228,2014-02-28,2014-02-28 10:00:00.123), (20160229,2016-02-29,2016-02-29 08:08:08.999), (20171031,2017-10-31,2017-12-31 11:59:59.123), (20191130,2019-11-30,2019-08-31 00:00:00.000))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List((20140228,2014-02-28,2014-02-28 10:00:00.123), (20160229,2016-02-29,2016-02-29 08:08:08.999), (20171031,2017-10-31,2017-12-31 11:59:59.123), (20191130,2019-11-30,2019-08-31 00:00:00.000))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimes = List((20140228, \"2014-02-28\", \"2014-02-28 10:00:00.123\"),\n",
    "                     (20160229, \"2016-02-29\", \"2016-02-29 08:08:08.999\"),\n",
    "                     (20171031, \"2017-10-31\", \"2017-12-31 11:59:59.123\"),\n",
    "                     (20191130, \"2019-11-30\", \"2019-08-31 00:00:00.000\")\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetimesDF = [date_id: int, date: string ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[date_id: int, date: string ... 1 more field]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datetimesDF = datetimes.toDF(\"date_id\", \"date\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+\n",
      "|date_id |date      |time                   |\n",
      "+--------+----------+-----------------------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|\n",
      "+--------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get unix timestamp for dateid, date and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------------+------------+----------+----------+\n",
      "|date_id |date      |time                   |unix_date_id|unix_date |unix_time |\n",
      "+--------+----------+-----------------------+------------+----------+----------+\n",
      "|20140228|2014-02-28|2014-02-28 10:00:00.123|1393563600  |1393563600|1393599600|\n",
      "|20160229|2016-02-29|2016-02-29 08:08:08.999|1456722000  |1456722000|1456751288|\n",
      "|20171031|2017-10-31|2017-12-31 11:59:59.123|1509422400  |1509422400|1514739599|\n",
      "|20191130|2019-11-30|2019-08-31 00:00:00.000|1575090000  |1575090000|1567224000|\n",
      "+--------+----------+-----------------------+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimesDF.\n",
    "    withColumn(\"unix_date_id\", unix_timestamp($\"date_id\".cast(\"string\"), \"yyyyMMdd\")).\n",
    "    withColumn(\"unix_date\", unix_timestamp($\"date\", \"yyyy-MM-dd\")).\n",
    "    withColumn(\"unix_time\", unix_timestamp($\"time\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a Dataframe by name unixtimesDF with one column unixtime using 4 values. You can use the unix timestamp generated for time column in previous task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unixtimes = List(1393561800, 1456713488, 1514701799, 1567189800)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "List(1393561800, 1456713488, 1514701799, 1567189800)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unixtimes = List(1393561800,\n",
    "                     1456713488,\n",
    "                     1514701799,\n",
    "                     1567189800\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unixtimesDF = [unixtime: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[unixtime: int]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val unixtimesDF = unixtimes.toDF(\"unixtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  unixtime|\n",
      "+----------+\n",
      "|1393561800|\n",
      "|1456713488|\n",
      "|1514701799|\n",
      "|1567189800|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|unixtime  |date_from_unix     |\n",
      "+----------+-------------------+\n",
      "|1393561800|2014-02-27 20:30:00|\n",
      "|1456713488|2016-02-28 18:38:08|\n",
      "|1514701799|2017-12-30 22:29:59|\n",
      "|1567189800|2019-08-30 11:30:00|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.withColumn(\"date_from_unix\", from_unixtime(col(\"unixtime\"))).show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get date in yyyyMMdd format and also complete timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------------------+\n",
      "|unixtime  |date    |time               |\n",
      "+----------+--------+-------------------+\n",
      "|1393561800|20140227|2014-02-27 23:30:00|\n",
      "|1456713488|20160228|2016-02-28 21:38:08|\n",
      "|1514701799|20171231|2017-12-31 01:29:59|\n",
      "|1567189800|20190830|2019-08-30 14:30:00|\n",
      "+----------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unixtimesDF.\n",
    "    withColumn(\"date\", from_unixtime($\"unixtime\", \"yyyyMMdd\")).\n",
    "    withColumn(\"time\", from_unixtime($\"unixtime\")).\n",
    "    show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As part of this module we have gone through list of functions that can be applied on top of columns for row level transformations.\n",
    "\n",
    "* There are approximately 300 pre-defined functions.\n",
    "* Functions can be broadly categorized into String Manipulation Functions, Date Manipulation Functions, Numeric Functions etc.\n",
    "* Typically when we read data from source, we get data in the form of strings and we need to apply functions to apply standardization rules, data type conversion, transformation rules etc.\n",
    "* Most of these functions can be used while projection using `select`, `selectExpr`, `withColumn` etc as well as part of `filter` or `where`, `groupBy`, `orderBy` or `sort` etc.\n",
    "* For `selectExpr` we need to use the functions using SQL Style syntax.\n",
    "* There are special functions such as `col` and `lit`. `col` is used to pass column names as column type for some of the functions while `lit` is used to pass literals as values as part of expressions (eg: `concat($\"first_name\", lit(\", \"), $\"last_name\")`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "\n",
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
